# LLaMA Text Summarizer (Ollama + FastAPI + Streamlit)

A full-stack AI application that generates concise summaries using a locally hosted LLaMA model via Ollama.

This project demonstrates how to integrate a Large Language Model (LLM) into a modern backendâ€“frontend architecture using FastAPI and Streamlit.

---

## ğŸš€ Project Overview

This application allows users to input large text and receive a short, meaningful summary generated by a locally running LLaMA model.

The system architecture:

User â†’ Streamlit (Frontend) â†’ FastAPI (Backend) â†’ Ollama â†’ LLaMA Model

---

## ğŸ› ï¸ Tech Stack

- **Ollama** â€“ Local LLaMA model hosting  
- **LLaMA 2** â€“ Large Language Model for summarization  
- **FastAPI** â€“ Backend API service  
- **Streamlit** â€“ Frontend user interface  
- **Python** â€“ Core programming language  
- **Git & GitHub** â€“ Version control  

---

## ğŸ“‚ Project Structure

```

text-summarizer-llama/
â”‚
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/aqsa-hafeez/llama-text-summarizer.git
cd llama-text-summarizer
````

---

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
```

Activate:

**Windows**

```bash
venv\Scripts\activate
```

**Mac/Linux**

```bash
source venv/bin/activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Install Ollama

Download from:
ğŸ‘‰ [https://ollama.com](https://ollama.com)

Pull the LLaMA model:

```bash
ollama pull llama2
```

Verify:

```bash
ollama list
```

---

## â–¶ï¸ Running the Application

### Step 1: Start Backend

```bash
uvicorn backend.main:app --reload
```

Backend will run at:

```
http://127.0.0.1:8000
```

---

### Step 2: Start Frontend (New Terminal)

```bash
streamlit run frontend/app.py
```

Frontend will open at:

```
http://localhost:8501
```

---

## ğŸ“Œ How It Works

1. User enters text in Streamlit UI
2. Streamlit sends request to FastAPI backend
3. FastAPI forwards prompt to Ollama
4. Ollama runs LLaMA model locally
5. Summary is returned and displayed

---
